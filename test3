import tensorflow as tf
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Dense
from keras.wrappers.scikit_learn import KerasClassifier
from sklearn import preprocessing
from tensorflow import keras
from tensorflow.keras import layers
from keras.utils import np_utils
import pandas as pd
import os
from sklearn.neighbors import KNeighborsClassifier
import numpy as np
from sklearn.model_selection import train_test_split, KFold, cross_val_score
from sklearn.metrics import accuracy_score
from sklearn.naive_bayes import GaussianNB
import random
import math
#from scipy import stats
from sklearn.metrics import accuracy_score
# from sklearn.neighbors import KNeighborsRegressor
# from sklearn.metrics import explained_variance_score
# from sklearn import preprocessing
# from numpy import nan
# from sklearn import tree
# from sklearn import preprocessing
# from sklearn import utils
from sklearn import tree
#import graphviz
print(tf.__version__)


####################################### data1 reading of data files##############################################
names =['sepal_length', 'sepal_width', 'petal_length', 'petal_width','species']

data1 = pd.read_csv("iris.data", header=None, skipinitialspace=True,
                    names=names, na_values=["?"])
# print(" The accuracy for data 1 is: ")
# # print(data1)

####################################### Turn data1 into category and cat.codes#############################################

data1["species"] = data1["species"].astype('category')
data1["species"] = data1["species"].cat.codes
# print(data1)

################### Part 1 IRIS #############################################

features = data1[['sepal_length', 'sepal_width', 'petal_length', 'petal_width']]
targets = data1['species']

dummy_y = np_utils.to_categorical(targets)
targets = dummy_y

train_data, test_data, train_targets, test_targets = train_test_split(features, targets, test_size=.3)

min_max_scaler = preprocessing.MinMaxScaler() # Allows normalization/scaling to happen for the data

# train_data = train_data.to_numpy() # Here we converted our data to a dataframe
train_data = min_max_scaler.fit_transform(train_data) # Normalizes/Scales the train_data

# train_targets = train_targets.to_numpy()

# test_data = test_data.to_numpy()
test_data = min_max_scaler.transform(test_data)



######################### Neural Network #############################################################

# Already imported tf.keras.layers so we can just put "Sequential" and "Dense"
classifier = Sequential()
classifier.add(Dense(100,activation="relu"))
classifier.add(Dense(80,activation="relu"))
classifier.add(Dense(60,activation="relu"))
classifier.add(Dense(50,activation="relu"))
classifier.add(Dense(3,activation="softmax")) # 3 different nodes where it corresponds to each class
classifier.compile(optimizer='Adam',loss=tf.keras.losses.categorical_crossentropy, metrics=["accuracy"])
# list of optimizers: SGD
# https://keras.io/api/optimizers/
# RMSprop
# Adam
# Adadelta
# Adagrad
# Adamax
# Nadam
# Ftrl

# list of Loss
# https://keras.io/api/losses/

classifier.fit(train_data,train_targets, epochs=100)

prediction = classifier.evaluate(test_data,test_targets)
print("Accuracy: %.2f%%" % (prediction[1]*100))



